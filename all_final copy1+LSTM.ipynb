{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sklearn\n",
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import MobileNetV3Large\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "def process_all_datasets(base_path, snr_min, snr_max, window_size, sequence_length):\n",
    "    X_sequences = []\n",
    "    y_sequences = []\n",
    "    for label_name in [chr(i) for i in range(ord(\"A\"), ord(\"C\")+1)]:\n",
    "        label_path = os.path.join(base_path, label_name)\n",
    "        if not os.path.exists(label_path):\n",
    "            print(f\"[WARNING] Label {label_name} not found, skipping.\")\n",
    "            continue\n",
    "\n",
    "        for person_name in os.listdir(label_path):\n",
    "            person_path = os.path.join(label_path, person_name)\n",
    "            if not os.path.isdir(person_path):\n",
    "                continue\n",
    "\n",
    "            for root, _, files in os.walk(person_path):\n",
    "                for dataset_file in files:\n",
    "                    if not dataset_file.endswith(\".csv\"):\n",
    "                        continue\n",
    "                    dataset_path = os.path.join(root, dataset_file)\n",
    "                    try:\n",
    "                        df = pd.read_csv(dataset_path)\n",
    "                        required = {\"timestamp\", \"doppler\", \"SNR\", \"x\", \"y\", \"z\"}\n",
    "                        if not required.issubset(df.columns):\n",
    "                            raise ValueError(f\"{dataset_path} missing {required}\")\n",
    "\n",
    "                        df[\"SNR\"] = np.clip(df[\"SNR\"], snr_min, snr_max)\n",
    "                        df[\"SNR\"] = np.log1p(df[\"SNR\"])\n",
    "                        unique_timestamps = np.sort(df[\"timestamp\"].unique())\n",
    "\n",
    "                        heatmaps = []\n",
    "\n",
    "                        for window_idx in range(len(unique_timestamps) - window_size + 1):\n",
    "                            t_subset = unique_timestamps[window_idx:window_idx+window_size]\n",
    "                            df_subset = df[df[\"timestamp\"].isin(t_subset)]\n",
    "                            if df_subset.empty:\n",
    "                                continue\n",
    "\n",
    "                            num_bins = 100\n",
    "                            x_bins = np.linspace(df_subset[\"x\"].min(), df_subset[\"x\"].max(), num_bins)\n",
    "                            y_bins = np.linspace(df_subset[\"y\"].min(), df_subset[\"y\"].max(), num_bins)\n",
    "                            z_bins = np.linspace(df_subset[\"z\"].min(), df_subset[\"z\"].max(), num_bins)\n",
    "                            dop_bins = np.linspace(df_subset[\"doppler\"].min(), df_subset[\"doppler\"].max(), num_bins)\n",
    "\n",
    "                            def resize(img):\n",
    "                                img_pil = Image.fromarray(img.astype(np.uint8))\n",
    "                                img_resized = img_pil.resize((64, 64), Image.LANCZOS)\n",
    "                                return np.array(img_resized)\n",
    "\n",
    "                            def make_heatmap(x, y, bx, by):\n",
    "                                h, _, _ = np.histogram2d(x, y, bins=[bx, by], weights=df_subset[\"SNR\"])\n",
    "                                hc, _, _ = np.histogram2d(x, y, bins=[bx, by])\n",
    "                                hc[hc == 0] = 1\n",
    "                                h /= hc\n",
    "                                return h.T\n",
    "\n",
    "                            dr = resize(make_heatmap(df_subset[\"doppler\"], df_subset[\"x\"], dop_bins, x_bins))\n",
    "                            dt = resize(make_heatmap(df_subset[\"doppler\"], df_subset[\"y\"], dop_bins, y_bins))\n",
    "                            dz = resize(make_heatmap(df_subset[\"doppler\"], df_subset[\"z\"], dop_bins, z_bins))\n",
    "\n",
    "                            heatmap_rgb = np.stack([dr, dt, dz], axis=-1)\n",
    "                            heatmap_rgb = (heatmap_rgb - heatmap_rgb.min()) / (heatmap_rgb.max() - 1e-8)\n",
    "                            heatmaps.append(heatmap_rgb)\n",
    "\n",
    "                        # Split the full heatmap list into sequences for LSTM\n",
    "                        for i in range(0, len(heatmaps) - sequence_length + 1):\n",
    "                            seq = heatmaps[i:i+sequence_length]\n",
    "                            X_sequences.append(seq)\n",
    "                            y_sequences.append(ord(label_name) - ord('A'))\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(f\"[ERROR] {dataset_path}: {e}\")\n",
    "\n",
    "    return np.array(X_sequences), np.array(y_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] Total saved: 60 sequences\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "# --- SETTINGS ---\n",
    "SNR_MIN = 4\n",
    "SNR_MAX = 843\n",
    "WINDOW_SIZE = 5         # frames per heatmap\n",
    "SEQUENCE_LENGTH = 30    # heatmaps per sequence\n",
    "OUTPUT_DIR = \"processed_dataset\"\n",
    "\n",
    "LABELS = [chr(ord(\"A\") + i) for i in range(2)]\n",
    "\n",
    "def resize(img):\n",
    "    img_pil = Image.fromarray(img.astype(np.uint8))\n",
    "    return np.array(img_pil.resize((64, 64), Image.LANCZOS))\n",
    "\n",
    "def save_sequence(X_seq, y_seq, count):\n",
    "    np.save(f\"{OUTPUT_DIR}/X_seq_{count}.npy\", X_seq.astype(np.float32))\n",
    "    np.save(f\"{OUTPUT_DIR}/y_seq_{count}.npy\", np.array(y_seq))\n",
    "\n",
    "def process_all(base_path):\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    count = 0\n",
    "\n",
    "    for label_name in LABELS:\n",
    "        label_path = os.path.join(base_path, label_name)\n",
    "        if not os.path.exists(label_path): continue\n",
    "\n",
    "        for person_name in os.listdir(label_path):\n",
    "            person_path = os.path.join(label_path, person_name)\n",
    "            if not os.path.isdir(person_path): continue\n",
    "\n",
    "            for root, _, files in os.walk(person_path):\n",
    "                for f in files:\n",
    "                    if not f.endswith(\".csv\"): continue\n",
    "                    df = pd.read_csv(os.path.join(root, f))\n",
    "\n",
    "                    if not {\"timestamp\", \"doppler\", \"x\", \"y\", \"z\", \"SNR\"}.issubset(df.columns):\n",
    "                        continue\n",
    "\n",
    "                    df[\"SNR\"] = np.clip(df[\"SNR\"], SNR_MIN, SNR_MAX)\n",
    "                    df[\"SNR\"] = np.log1p(df[\"SNR\"])\n",
    "\n",
    "                    timestamps = np.sort(df[\"timestamp\"].unique())\n",
    "\n",
    "                    heatmaps = []\n",
    "                    for i in range(len(timestamps) - WINDOW_SIZE + 1):\n",
    "                        t_win = timestamps[i:i+WINDOW_SIZE]\n",
    "                        df_win = df[df[\"timestamp\"].isin(t_win)]\n",
    "\n",
    "                        def make_map(x, y):\n",
    "                            h, _, _ = np.histogram2d(x, y, bins=50, weights=df_win[\"SNR\"])\n",
    "                            c, _, _ = np.histogram2d(x, y, bins=50)\n",
    "                            c[c == 0] = 1\n",
    "                            h /= c\n",
    "                            return resize(h.T)\n",
    "\n",
    "                        dr = make_map(df_win[\"doppler\"], df_win[\"x\"])\n",
    "                        dt = make_map(df_win[\"doppler\"], df_win[\"y\"])\n",
    "                        dz = make_map(df_win[\"doppler\"], df_win[\"z\"])\n",
    "\n",
    "                        rgb = np.stack([dr, dt, dz], axis=-1)\n",
    "                        rgb = (rgb - rgb.min()) / (rgb.max() - rgb.min() + 1e-8)\n",
    "\n",
    "                        heatmaps.append(rgb)\n",
    "\n",
    "                        if len(heatmaps) == SEQUENCE_LENGTH:\n",
    "                            X_seq = np.stack(heatmaps, axis=0)\n",
    "                            label_idx = LABELS.index(label_name)\n",
    "                            y_seq = [label_idx] * SEQUENCE_LENGTH\n",
    "\n",
    "                            save_sequence(X_seq, y_seq, count)\n",
    "                            count += 1\n",
    "                            heatmaps = []\n",
    "\n",
    "    print(f\"[DONE] Total saved: {count} sequences\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_all(\"dataset 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "class RadarSequence(Sequence):\n",
    "    def __init__(self, X_data, y_data, batch_size=4, shuffle=True):\n",
    "        \"\"\"\n",
    "        A robust Keras Sequence for radar heatmap + LSTM training.\n",
    "        \n",
    "        Args:\n",
    "            X_data (np.array): shape (num_samples, seq_len, 64, 64, 3)\n",
    "            y_data (np.array): shape (num_samples,)\n",
    "            batch_size (int): batch size\n",
    "            shuffle (bool): whether to shuffle indexes each epoch\n",
    "        \"\"\"\n",
    "        self.X = X_data\n",
    "        self.y = y_data\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.indexes = np.arange(len(self.X))\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Number of batches per epoch\n",
    "        \"\"\"\n",
    "        return int(np.ceil(len(self.X) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Generate one batch\n",
    "        \"\"\"\n",
    "        batch_indexes = self.indexes[idx * self.batch_size : (idx + 1) * self.batch_size]\n",
    "        X_batch = self.X[batch_indexes]\n",
    "        y_batch = self.y[batch_indexes]\n",
    "        return X_batch, y_batch\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        \"\"\"\n",
    "        Shuffle indexes after each epoch\n",
    "        \"\"\"\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sequences found: 60\n",
      "Train sequences: 48, Test sequences: 12\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import glob\n",
    "\n",
    "# 1️⃣ Get all sequence file names\n",
    "X_files = sorted(glob.glob(f\"{OUTPUT_DIR}/X_seq_*.npy\"))\n",
    "y_files = sorted(glob.glob(f\"{OUTPUT_DIR}/y_seq_*.npy\"))\n",
    "\n",
    "print(f\"Total sequences found: {len(X_files)}\")\n",
    "\n",
    "# 2️⃣ Split file paths (not data yet!)\n",
    "X_train_files, X_test_files, y_train_files, y_test_files = train_test_split(\n",
    "    X_files, y_files, test_size=0.2, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "print(f\"Train sequences: {len(X_train_files)}, Test sequences: {len(X_test_files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total dataset setelah diproses: 60\n",
      "Train Files: 48\n",
      "Test Files: 12\n",
      "Label unik sebelum split: ['processed_dataset\\\\y_seq_0.npy' 'processed_dataset\\\\y_seq_1.npy'\n",
      " 'processed_dataset\\\\y_seq_10.npy' 'processed_dataset\\\\y_seq_11.npy'\n",
      " 'processed_dataset\\\\y_seq_12.npy' 'processed_dataset\\\\y_seq_13.npy'\n",
      " 'processed_dataset\\\\y_seq_14.npy' 'processed_dataset\\\\y_seq_15.npy'\n",
      " 'processed_dataset\\\\y_seq_16.npy' 'processed_dataset\\\\y_seq_17.npy'\n",
      " 'processed_dataset\\\\y_seq_18.npy' 'processed_dataset\\\\y_seq_19.npy'\n",
      " 'processed_dataset\\\\y_seq_2.npy' 'processed_dataset\\\\y_seq_20.npy'\n",
      " 'processed_dataset\\\\y_seq_21.npy' 'processed_dataset\\\\y_seq_22.npy'\n",
      " 'processed_dataset\\\\y_seq_23.npy' 'processed_dataset\\\\y_seq_24.npy'\n",
      " 'processed_dataset\\\\y_seq_25.npy' 'processed_dataset\\\\y_seq_26.npy'\n",
      " 'processed_dataset\\\\y_seq_27.npy' 'processed_dataset\\\\y_seq_28.npy'\n",
      " 'processed_dataset\\\\y_seq_29.npy' 'processed_dataset\\\\y_seq_3.npy'\n",
      " 'processed_dataset\\\\y_seq_30.npy' 'processed_dataset\\\\y_seq_31.npy'\n",
      " 'processed_dataset\\\\y_seq_32.npy' 'processed_dataset\\\\y_seq_33.npy'\n",
      " 'processed_dataset\\\\y_seq_34.npy' 'processed_dataset\\\\y_seq_35.npy'\n",
      " 'processed_dataset\\\\y_seq_36.npy' 'processed_dataset\\\\y_seq_37.npy'\n",
      " 'processed_dataset\\\\y_seq_38.npy' 'processed_dataset\\\\y_seq_39.npy'\n",
      " 'processed_dataset\\\\y_seq_4.npy' 'processed_dataset\\\\y_seq_40.npy'\n",
      " 'processed_dataset\\\\y_seq_41.npy' 'processed_dataset\\\\y_seq_42.npy'\n",
      " 'processed_dataset\\\\y_seq_43.npy' 'processed_dataset\\\\y_seq_44.npy'\n",
      " 'processed_dataset\\\\y_seq_45.npy' 'processed_dataset\\\\y_seq_46.npy'\n",
      " 'processed_dataset\\\\y_seq_47.npy' 'processed_dataset\\\\y_seq_48.npy'\n",
      " 'processed_dataset\\\\y_seq_49.npy' 'processed_dataset\\\\y_seq_5.npy'\n",
      " 'processed_dataset\\\\y_seq_50.npy' 'processed_dataset\\\\y_seq_51.npy'\n",
      " 'processed_dataset\\\\y_seq_52.npy' 'processed_dataset\\\\y_seq_53.npy'\n",
      " 'processed_dataset\\\\y_seq_54.npy' 'processed_dataset\\\\y_seq_55.npy'\n",
      " 'processed_dataset\\\\y_seq_56.npy' 'processed_dataset\\\\y_seq_57.npy'\n",
      " 'processed_dataset\\\\y_seq_58.npy' 'processed_dataset\\\\y_seq_59.npy'\n",
      " 'processed_dataset\\\\y_seq_6.npy' 'processed_dataset\\\\y_seq_7.npy'\n",
      " 'processed_dataset\\\\y_seq_8.npy' 'processed_dataset\\\\y_seq_9.npy']\n",
      "Label unik di y_train: ['processed_dataset\\\\y_seq_1.npy' 'processed_dataset\\\\y_seq_10.npy'\n",
      " 'processed_dataset\\\\y_seq_11.npy' 'processed_dataset\\\\y_seq_12.npy'\n",
      " 'processed_dataset\\\\y_seq_14.npy' 'processed_dataset\\\\y_seq_15.npy'\n",
      " 'processed_dataset\\\\y_seq_16.npy' 'processed_dataset\\\\y_seq_17.npy'\n",
      " 'processed_dataset\\\\y_seq_18.npy' 'processed_dataset\\\\y_seq_19.npy'\n",
      " 'processed_dataset\\\\y_seq_21.npy' 'processed_dataset\\\\y_seq_22.npy'\n",
      " 'processed_dataset\\\\y_seq_23.npy' 'processed_dataset\\\\y_seq_24.npy'\n",
      " 'processed_dataset\\\\y_seq_25.npy' 'processed_dataset\\\\y_seq_26.npy'\n",
      " 'processed_dataset\\\\y_seq_27.npy' 'processed_dataset\\\\y_seq_28.npy'\n",
      " 'processed_dataset\\\\y_seq_29.npy' 'processed_dataset\\\\y_seq_3.npy'\n",
      " 'processed_dataset\\\\y_seq_30.npy' 'processed_dataset\\\\y_seq_31.npy'\n",
      " 'processed_dataset\\\\y_seq_32.npy' 'processed_dataset\\\\y_seq_33.npy'\n",
      " 'processed_dataset\\\\y_seq_34.npy' 'processed_dataset\\\\y_seq_35.npy'\n",
      " 'processed_dataset\\\\y_seq_36.npy' 'processed_dataset\\\\y_seq_37.npy'\n",
      " 'processed_dataset\\\\y_seq_38.npy' 'processed_dataset\\\\y_seq_4.npy'\n",
      " 'processed_dataset\\\\y_seq_40.npy' 'processed_dataset\\\\y_seq_42.npy'\n",
      " 'processed_dataset\\\\y_seq_43.npy' 'processed_dataset\\\\y_seq_44.npy'\n",
      " 'processed_dataset\\\\y_seq_45.npy' 'processed_dataset\\\\y_seq_46.npy'\n",
      " 'processed_dataset\\\\y_seq_47.npy' 'processed_dataset\\\\y_seq_48.npy'\n",
      " 'processed_dataset\\\\y_seq_49.npy' 'processed_dataset\\\\y_seq_51.npy'\n",
      " 'processed_dataset\\\\y_seq_53.npy' 'processed_dataset\\\\y_seq_55.npy'\n",
      " 'processed_dataset\\\\y_seq_56.npy' 'processed_dataset\\\\y_seq_57.npy'\n",
      " 'processed_dataset\\\\y_seq_59.npy' 'processed_dataset\\\\y_seq_6.npy'\n",
      " 'processed_dataset\\\\y_seq_8.npy' 'processed_dataset\\\\y_seq_9.npy']\n",
      "Label unik di y_test: ['processed_dataset\\\\y_seq_0.npy' 'processed_dataset\\\\y_seq_13.npy'\n",
      " 'processed_dataset\\\\y_seq_2.npy' 'processed_dataset\\\\y_seq_20.npy'\n",
      " 'processed_dataset\\\\y_seq_39.npy' 'processed_dataset\\\\y_seq_41.npy'\n",
      " 'processed_dataset\\\\y_seq_5.npy' 'processed_dataset\\\\y_seq_50.npy'\n",
      " 'processed_dataset\\\\y_seq_52.npy' 'processed_dataset\\\\y_seq_54.npy'\n",
      " 'processed_dataset\\\\y_seq_58.npy' 'processed_dataset\\\\y_seq_7.npy']\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_files, y_files, test_size=0.2, random_state=42)\n",
    "# Tampilkan hasil\n",
    "print(\"Total dataset setelah diproses:\", len(X_files))\n",
    "print(\"Train Files:\", len(X_train))\n",
    "print(\"Test Files:\", len(X_test))\n",
    "# Cek label unik sebelum dan setelah split\n",
    "print(\"Label unik sebelum split:\", np.unique(y_files))\n",
    "print(\"Label unik di y_train:\", np.unique(y_train))\n",
    "print(\"Label unik di y_test:\", np.unique(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_all' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[0;32m      3\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(\n\u001b[1;32m----> 4\u001b[0m     \u001b[43mX_all\u001b[49m, y_all, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m\n\u001b[0;32m      5\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_all' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install keras-tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_4\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_4\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ time_distributed_20             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>) │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ time_distributed_21             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>) │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ time_distributed_22             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>) │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ time_distributed_23             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>) │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ time_distributed_24             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16384</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">8,454,656</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">387</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ time_distributed_20             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m32\u001b[0m) │           \u001b[38;5;34m896\u001b[0m │\n",
       "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ time_distributed_21             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m) │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ time_distributed_22             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m) │        \u001b[38;5;34m18,496\u001b[0m │\n",
       "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ time_distributed_23             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m64\u001b[0m) │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ time_distributed_24             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m16384\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_4 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │     \u001b[38;5;34m8,454,656\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │           \u001b[38;5;34m387\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8,474,435</span> (32.33 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m8,474,435\u001b[0m (32.33 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8,474,435</span> (32.33 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m8,474,435\u001b[0m (32.33 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, LSTM, TimeDistributed, Dropout, BatchNormalization\n",
    "#from radar_loader import RadarSequence\n",
    "\n",
    "input_shape = (30, 64, 64, 3)  # SEQUENCE, H, W, C\n",
    "\n",
    "model = Sequential([\n",
    "    TimeDistributed(Conv2D(32, (3,3), activation='relu', padding='same'), input_shape=input_shape),\n",
    "    TimeDistributed(MaxPooling2D(2,2)),\n",
    "    TimeDistributed(Conv2D(64, (3,3), activation='relu', padding='same')),\n",
    "    TimeDistributed(MaxPooling2D(2,2)),\n",
    "    TimeDistributed(Flatten()),\n",
    "    LSTM(128, return_sequences=False),\n",
    "    Dropout(0.4),\n",
    "    Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Core i3\\Documents\\iqbal\\TAIQBAL\\TAIQBAL\\temp-main\\.iqbal\\lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "only integer scalar arrays can be converted to a scalar index",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[62], line 24\u001b[0m\n\u001b[0;32m     16\u001b[0m early_stop \u001b[38;5;241m=\u001b[39m EarlyStopping(\n\u001b[0;32m     17\u001b[0m     monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     18\u001b[0m     patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,\n\u001b[0;32m     19\u001b[0m     restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     20\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     21\u001b[0m )\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# ✅ Fit using the generator — do NOT pass (X_train, y_train) directly!\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_seq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_seq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stop\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[0;32m     30\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# ✅ Predict all batches in test_seq\u001b[39;00m\n\u001b[0;32m     33\u001b[0m y_true \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\Core i3\\Documents\\iqbal\\TAIQBAL\\TAIQBAL\\temp-main\\.iqbal\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[1;32mIn[55], line 33\u001b[0m, in \u001b[0;36mRadarSequence.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;124;03mGenerate one batch\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     32\u001b[0m batch_indexes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindexes[idx \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size : (idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size]\n\u001b[1;32m---> 33\u001b[0m X_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbatch_indexes\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     34\u001b[0m y_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my[batch_indexes]\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X_batch, y_batch\n",
      "\u001b[1;31mTypeError\u001b[0m: only integer scalar arrays can be converted to a scalar index"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 👇 Your DataSequence class must be defined before this, e.g.:\n",
    "# train_seq = RadarSequence(X_train_files, y_train_files, batch_size=2)\n",
    "# test_seq = RadarSequence(X_test_files, y_test_files, batch_size=2)\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "train_seq = RadarSequence(X_train, y_train, batch_size=batch_size)\n",
    "test_seq  = RadarSequence(X_test, y_test, batch_size=batch_size, shuffle=False)\n",
    "# ✅ Early Stopping\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ✅ Fit using the generator — do NOT pass (X_train, y_train) directly!\n",
    "history = model.fit(\n",
    "    train_seq,\n",
    "    validation_data=test_seq,\n",
    "    epochs=10,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ✅ Predict all batches in test_seq\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "# --- Predict in ONE shot ---\n",
    "y_pred_prob = model.predict(test_seq)  # shape: (num_samples, num_classes)\n",
    "\n",
    "y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "\n",
    "# --- True labels ---\n",
    "y_true = []\n",
    "for _, y_batch in test_seq:\n",
    "    y_true.extend(y_batch)\n",
    "\n",
    "y_true = np.array(y_true)\n",
    "\n",
    "print(classification_report(y_true, y_pred))\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize=(12, 4))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=[\"A\", \"B\", \"C\"],\n",
    "            yticklabels=[\"A\", \"B\", \"C\"])\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# ✅ Train vs Validation Plots\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Train vs Validation Loss')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Train vs Validation Accuracy')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "#!rm -rf ./logs/\n",
    "\n",
    "logdir=\"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir, histogram_freq=1, write_graph=True)\n",
    "\n",
    "\n",
    "# Training model\n",
    "history = tuner.search(\n",
    "    X_train, y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    class_weight=class_weights,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=[early_stop,tensorboard_callback],\n",
    "    verbose=1\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_trial = tuner.oracle.get_best_trials(num_trials = 1)[0]\n",
    "\n",
    "print(best_trial.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install visualkeras \n",
    "!pip install --upgrade visualkeras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bestmodel = tuner.get_best_models(1)[0]\n",
    "bestmodel.save(\"a-z(30).h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = tuner.get_best_models(1)[0]\n",
    "\n",
    "\n",
    "\n",
    "y_pred = best_model.predict(X_test) \n",
    "y_pred_classes = y_pred.argmax(axis=1)  \n",
    "print(classification_report(y_test, y_pred_classes))\n",
    "cm = confusion_matrix(y_test, y_pred_classes)\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['A', 'B', 'C','D','E','F','G','H','I','J','K','L','M','N','O','P','Q','R','S','T','U','V','W','X','Y','Z'],\n",
    "            yticklabels=['A', 'B', 'C','D','E','F','G','H','I','J','K','L','M','N','O','P','Q','R','S','T','U','V','W','X','Y','Z'])\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "best_hp = tuner.get_best_hyperparameters(1)[0]\n",
    "best_model = tuner.hypermodel.build(best_hp)\n",
    "\n",
    "history = best_model.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    class_weight=class_weights,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=[early_stop,tensorboard_callback],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Train vs Validation Loss')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Train vs Validation Accuracy')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_mod = tuner.get_best_models(1)[0]\n",
    "\n",
    "best_mod.save(\"random.h5\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import visualkeras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "# Assuming your model is built using `tuner.get_best_models(1)[0]`\n",
    "model = tuner.get_best_models(1)[0]\n",
    "\n",
    "\n",
    "\n",
    "visualkeras.layered_view(model, legend=True, show_dimension=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"coba_coba2.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".iqbal (3.10.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
