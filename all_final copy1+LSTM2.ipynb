{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sklearn\n",
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import MobileNetV3Large\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import TimeDistributed, Conv2D, MaxPooling2D, Flatten, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import Sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_datasets(base_path, snr_min, snr_max, window_size):\n",
    "    X_sequences = []\n",
    "    y_sequences = []\n",
    "\n",
    "    labels = list(\"ABC\")\n",
    "\n",
    "    for label_name in labels:\n",
    "        label_path = os.path.join(base_path, label_name)\n",
    "        if not os.path.exists(label_path):\n",
    "            print(f\"[WARNING] Label {label_name} not found, skipped.\")\n",
    "            continue\n",
    "\n",
    "        for person_name in os.listdir(label_path):\n",
    "            person_path = os.path.join(label_path, person_name)\n",
    "            if not os.path.isdir(person_path):\n",
    "                continue\n",
    "\n",
    "            for root, _, files in os.walk(person_path):\n",
    "                for file in files:\n",
    "                    if not file.endswith(\".csv\"):\n",
    "                        continue\n",
    "\n",
    "                    df = pd.read_csv(os.path.join(root, file))\n",
    "                    df[\"SNR\"] = np.clip(df[\"SNR\"], snr_min, snr_max)\n",
    "                    df[\"SNR\"] = np.log1p(df[\"SNR\"])\n",
    "\n",
    "                    unique_ts = np.sort(df[\"timestamp\"].unique())\n",
    "\n",
    "                    for window_idx in range(len(unique_ts) - window_size + 1):\n",
    "                        ts_slice = unique_ts[window_idx : window_idx + window_size]\n",
    "                        df_slice = df[df[\"timestamp\"].isin(ts_slice)]\n",
    "\n",
    "                        if df_slice.empty:\n",
    "                            continue\n",
    "\n",
    "                        def resize(img):\n",
    "                            img = Image.fromarray(img.astype(np.uint8))\n",
    "                            img = img.resize((64, 64), Image.LANCZOS)\n",
    "                            return np.array(img)\n",
    "\n",
    "                        def make_heatmap(x, y, bins_x, bins_y):\n",
    "                            heatmap, _, _ = np.histogram2d(x, y, bins=[bins_x, bins_y], weights=df_slice[\"SNR\"])\n",
    "                            count, _, _ = np.histogram2d(x, y, bins=[bins_x, bins_y])\n",
    "                            count[count == 0] = 1\n",
    "                            heatmap = heatmap / count\n",
    "                            return heatmap.T\n",
    "\n",
    "                        num_bins = 100\n",
    "                        dop_bins = np.linspace(df_slice[\"doppler\"].min(), df_slice[\"doppler\"].max(), num_bins)\n",
    "                        x_bins = np.linspace(df_slice[\"x\"].min(), df_slice[\"x\"].max(), num_bins)\n",
    "                        y_bins = np.linspace(df_slice[\"y\"].min(), df_slice[\"y\"].max(), num_bins)\n",
    "                        z_bins = np.linspace(df_slice[\"z\"].min(), df_slice[\"z\"].max(), num_bins)\n",
    "\n",
    "                        dr = resize(make_heatmap(df_slice[\"doppler\"], df_slice[\"x\"], dop_bins, x_bins))\n",
    "                        dt = resize(make_heatmap(df_slice[\"doppler\"], df_slice[\"y\"], dop_bins, y_bins))\n",
    "                        dz = resize(make_heatmap(df_slice[\"doppler\"], df_slice[\"z\"], dop_bins, z_bins))\n",
    "\n",
    "                        heatmap = np.stack([dr, dt, dz], axis=-1)\n",
    "                        heatmap = (heatmap - heatmap.min()) / (heatmap.max() - heatmap.min() + 1e-8)\n",
    "\n",
    "                        X_sequences.append(heatmap)\n",
    "                        y_sequences.append(ord(label_name) - ord('A'))\n",
    "\n",
    "    X_sequences = np.array(X_sequences, dtype=np.float32)\n",
    "    y_sequences = np.array(y_sequences)\n",
    "    return X_sequences, y_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = process_all_datasets(\n",
    "    base_path=\"dataset 2\",\n",
    "    snr_min=4,\n",
    "    snr_max=843,\n",
    "    window_size=5\n",
    ")\n",
    "\n",
    "print(f\"âœ… X shape: {X.shape}\")\n",
    "print(f\"âœ… y shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 30\n",
    "X_seq = []\n",
    "y_seq = []\n",
    "\n",
    "for i in range(len(X) - seq_len):\n",
    "    X_seq.append(X[i:i+seq_len])\n",
    "    y_seq.append(y[i+seq_len-1])  # Last label\n",
    "\n",
    "X_seq = np.array(X_seq)\n",
    "y_seq = np.array(y_seq)\n",
    "\n",
    "print(f\"âœ… X_seq shape: {X_seq.shape}\")\n",
    "print(f\"âœ… y_seq shape: {y_seq.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_seq, y_seq, test_size=0.2, random_state=42, stratify=y_seq\n",
    ")\n",
    "\n",
    "print(f\"Train: {X_train.shape}, Test: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RadarSequence(Sequence):\n",
    "    def __init__(self, X, y, batch_size=30, shuffle=True):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.indexes = np.arange(len(self.X))\n",
    "        self.on_epoch_end()\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.X) / self.batch_size))\n",
    "    def __getitem__(self, idx):\n",
    "        batch_idx = self.indexes[idx * self.batch_size: (idx + 1) * self.batch_size]\n",
    "        return self.X[batch_idx], self.y[batch_idx]\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    TimeDistributed(Conv2D(16, (3,3), activation='relu', padding='same'),\n",
    "                    input_shape=X_train.shape[1:]),\n",
    "    TimeDistributed(MaxPooling2D((2,2))),\n",
    "    TimeDistributed(Conv2D(32, (3,3), activation='relu', padding='same')),\n",
    "    TimeDistributed(MaxPooling2D((2,2))),\n",
    "    TimeDistributed(Flatten()),\n",
    "    LSTM(64),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(len(np.unique(y)), activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.0005),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "train_seq = RadarSequence(X_train, y_train, batch_size=30)\n",
    "test_seq  = RadarSequence(X_test, y_test, batch_size=30, shuffle=False)\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(\n",
    "    train_seq,\n",
    "    validation_data=test_seq,\n",
    "    epochs=30,\n",
    "    callbacks=[early_stop],\n",
    "    class_weight=class_weights\n",
    ")\n",
    "\n",
    "# ============================================\n",
    "# ðŸ“Œ 9. Evaluate\n",
    "# ============================================\n",
    "\n",
    "y_pred = model.predict(test_seq)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='Train Acc')\n",
    "plt.plot(history.history['val_accuracy'], label='Val Acc')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('LSTM.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# ðŸ‘‡ Load your trained model (adjust filename)\n",
    "model = load_model(\"LSTM.h5\")  # or .keras or saved_model path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ‘‡ Example: assume you saved training heatmaps as numpy arrays\n",
    "# If not, load directly from your X_train:\n",
    "train_heatmap = X_train[0]\n",
    "\n",
    "print(\"âœ… Training heatmap shape:\", train_heatmap.shape)\n",
    "print(\"min:\", train_heatmap.min(), \"max:\", train_heatmap.max())\n",
    "plt.imshow(train_heatmap[:, :, 0])  # show first channel\n",
    "plt.title(\"Training - Channel 1 (Doppler-X)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ‘‡ Example: save live heatmap during run\n",
    "# np.save(\"live_heatmap.npy\", heatmap_rgb)\n",
    "\n",
    "live_heatmap = np.load(\"live_heatmap.npy\")\n",
    "\n",
    "print(\"âœ… Live heatmap shape:\", live_heatmap.shape)\n",
    "print(\"min:\", live_heatmap.min(), \"max:\", live_heatmap.max())\n",
    "plt.imshow(live_heatmap[:, :, 0])\n",
    "plt.title(\"Real-time - Channel 1 (Doppler-X)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "#!rm -rf ./logs/\n",
    "\n",
    "logdir=\"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir, histogram_freq=1, write_graph=True)\n",
    "\n",
    "\n",
    "# Training model\n",
    "history = tuner.search(\n",
    "    X_train, y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    class_weight=class_weights,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=[early_stop,tensorboard_callback],\n",
    "    verbose=1\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_trial = tuner.oracle.get_best_trials(num_trials = 1)[0]\n",
    "\n",
    "print(best_trial.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install visualkeras \n",
    "!pip install --upgrade visualkeras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bestmodel = tuner.get_best_models(1)[0]\n",
    "bestmodel.save(\"a-z(30).h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = tuner.get_best_models(1)[0]\n",
    "\n",
    "\n",
    "\n",
    "y_pred = best_model.predict(X_test) \n",
    "y_pred_classes = y_pred.argmax(axis=1)  \n",
    "print(classification_report(y_test, y_pred_classes))\n",
    "cm = confusion_matrix(y_test, y_pred_classes)\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['A', 'B', 'C','D','E','F','G','H','I','J','K','L','M','N','O','P','Q','R','S','T','U','V','W','X','Y','Z'],\n",
    "            yticklabels=['A', 'B', 'C','D','E','F','G','H','I','J','K','L','M','N','O','P','Q','R','S','T','U','V','W','X','Y','Z'])\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "best_hp = tuner.get_best_hyperparameters(1)[0]\n",
    "best_model = tuner.hypermodel.build(best_hp)\n",
    "\n",
    "history = best_model.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    class_weight=class_weights,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=[early_stop,tensorboard_callback],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Train vs Validation Loss')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Train vs Validation Accuracy')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_mod = tuner.get_best_models(1)[0]\n",
    "\n",
    "best_mod.save(\"random.h5\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import visualkeras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "# Assuming your model is built using `tuner.get_best_models(1)[0]`\n",
    "model = tuner.get_best_models(1)[0]\n",
    "\n",
    "\n",
    "\n",
    "visualkeras.layered_view(model, legend=True, show_dimension=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"coba_coba2.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
